{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: allenai/tulu-v2-sft-mixture\n",
      "Average input token length: 353.33\n",
      "Average output token length: 696.89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "def preprocess_get_input_and_output(dataset_name, max_samples=50000):\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # # Initialize lists to store input and output texts\n",
    "    input_texts = []\n",
    "    output_texts = []\n",
    "\n",
    "    # Process each dataset individually based on its format\n",
    "    if dataset_name == \"casey-martin/MedInstruct\":\n",
    "        for example in dataset[\"train\"].select(range(min(max_samples, len(dataset[\"train\"])))):\n",
    "            input_text = example[\"instruction\"] + example[\"input\"]\n",
    "            output_text = example[\"output\"]\n",
    "            input_texts.append(input_text)\n",
    "            output_texts.append(output_text)\n",
    "    elif dataset_name == \"TIGER-Lab/MathInstruct\":\n",
    "        for example in dataset[\"train\"].select(range(min(max_samples, len(dataset[\"train\"])))):\n",
    "            input_text = example[\"instruction\"]\n",
    "            output_text = example[\"output\"]\n",
    "            input_texts.append(input_text)\n",
    "            output_texts.append(output_text)\n",
    "    elif dataset_name == \"train_en_phy_chem.json\":\n",
    "        with open(dataset_name, 'r') as file:\n",
    "            for i, line in enumerate(file):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                example = json.loads(line)\n",
    "                input_text = example[\"content\"]\n",
    "                output_text = example[\"summary\"]\n",
    "                input_texts.append(input_text)\n",
    "                output_texts.append(output_text)\n",
    "    elif dataset_name == \"chemical_disease_interaction_extraction.json\":\n",
    "        with open(dataset_name, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Process each example in the JSON data\n",
    "            for example in data[:max_samples]:\n",
    "                input_text = example[\"instruction\"] + \" \" + example[\"input\"]\n",
    "                output_text = example[\"output\"]\n",
    "                input_texts.append(input_text)\n",
    "                output_texts.append(output_text)\n",
    "    elif dataset_name == 'osunlp/SMolInstruct':\n",
    "        for example in dataset[\"train\"].select(range(min(max_samples, len(dataset[\"train\"])))):\n",
    "            input_text = example[\"input\"]\n",
    "            output_text = example[\"output\"]\n",
    "            input_texts.append(input_text)\n",
    "            output_texts.append(output_text)\n",
    "    elif dataset_name == 'allenai/tulu-v2-sft-mixture':\n",
    "        for example in dataset[\"train\"].select(range(min(max_samples, len(dataset[\"train\"])))):\n",
    "            user_content = \"\"\n",
    "            assistant_content = \"\"\n",
    "\n",
    "            # Iterate over each message in the \"messages\" list\n",
    "            for message in example[\"messages\"]:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    user_content += message[\"content\"] + \"\\n\"\n",
    "                elif message[\"role\"] == \"assistant\":\n",
    "                    assistant_content += message[\"content\"] + \"\\n\"\n",
    "\n",
    "            input_texts.append(user_content.strip())\n",
    "            output_texts.append(assistant_content.strip())\n",
    "\n",
    "    return input_texts, output_texts\n",
    "    \n",
    "\n",
    "    return input_texts, output_texts\n",
    "    # Add more elif conditions for other datasets as needed\n",
    "\n",
    "    return input_texts, output_texts\n",
    "\n",
    "def compute_avg_token_length(tokenizer_model, input_texts, output_texts):\n",
    "    # Load the tokenizer\n",
    "    tokenizer = LlamaTokenizerFast.from_pretrained(tokenizer_model)\n",
    "\n",
    "    # Initialize variables to store total token lengths\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    # Tokenize and compute token lengths for the limited number of samples\n",
    "    for i in range(len(input_texts)):\n",
    "        input_tokens = tokenizer.encode(input_texts[i])\n",
    "        output_tokens = tokenizer.encode(output_texts[i])\n",
    "        total_input_tokens += len(input_tokens)\n",
    "        total_output_tokens += len(output_tokens)\n",
    "\n",
    "    # Compute average token lengths\n",
    "    avg_input_length = total_input_tokens / len(input_texts)\n",
    "    avg_output_length = total_output_tokens / len(output_texts)\n",
    "\n",
    "    return avg_input_length, avg_output_length\n",
    "\n",
    "# Example usage\n",
    "dataset_names = [\"allenai/tulu-v2-sft-mixture\"]\n",
    "tokenizer_model = \"NousResearch/Llama-2-7b-hf\"\n",
    "max_samples = 3000000\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    input_texts, output_texts = preprocess_get_input_and_output(dataset_name, max_samples)\n",
    "    avg_input_length, avg_output_length = compute_avg_token_length(tokenizer_model, input_texts, output_texts)\n",
    "\n",
    "    print(f\"Average input token length: {avg_input_length:.2f}\")\n",
    "    print(f\"Average output token length: {avg_output_length:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science-adapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
