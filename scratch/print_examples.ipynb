{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, numpy as np, pandas as pd, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['scierc_ner','scifact_entailment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be shown sections from a scientific research paper, together with a question about the paper. Paragraphs in the paper are separated by newlines. Your task is to answer the question based on the contents of the paper.\n",
      "\n",
      "Paper:\n",
      "----------------------------------------\n",
      "Fusing Visual, Textual and Connectivity Clues for Studying Mental Health\n",
      "\n",
      "With ubiquity of social media platforms, millions of people are sharing their online persona by expressing their thoughts, moods, emotions, feelings, and even their daily struggles with mental health issues voluntarily and publicly on social media. Unlike the most existing efforts which study depression by analyzing textual content, we examine and exploit multimodal big data to discern depressive behavior using a wide variety of features including individual-level demographics. By developing a multimodal framework and employing statistical techniques for fusing heterogeneous sets of features obtained by processing visual, textual and user interaction data, we significantly enhance the current state-of-the-art approaches for identifying depressed individuals on Twitter (improving the average F1-Score by 5 percent) as well as facilitate demographic inference from social media for broader applications. Besides providing insights into the relationship between demographics and mental health, our research assists in the design of a new breed of demographic-aware health interventions.\n",
      "\n",
      "Introduction\n",
      "Depression is a highly prevalent public health challenge and a major cause of disability worldwide. Depression affects 6.7% (i.e., about 16 million) Americans each year . According to the World Mental Health Survey conducted in 17 countries, on average, about 5% of people reported having an episode of depression in 2011 BIBREF0 . Untreated or under-treated clinical depression can lead to suicide and other chronic risky behaviors such as drug or alcohol addiction.\n",
      "Global efforts to curb clinical depression involve identifying depression through survey-based methods employing phone or online questionnaires. These approaches suffer from under-representation as well as sampling bias (with very small group of respondents.) In contrast, the widespread adoption of social media where people voluntarily and publicly express their thoughts, moods, emotions, and feelings, and even share their daily struggles with mental health problems has not been adequately tapped into studying mental illnesses, such as depression. The visual and textual content shared on different social media platforms like Twitter offer new opportunities for a deeper understanding of self-expressed depression both at an individual as well as community-level. Previous research efforts have suggested that language style, sentiment, users' activities, and engagement expressed in social media posts can predict the likelihood of depression BIBREF1 , BIBREF2 . However, except for a few attempts BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , these investigations have seldom studied extraction of emotional state from visual content of images in posted/profile images. Visual content can express users' emotions more vividly, and psychologists noted that imagery is an effective medium for communicating difficult emotions.\n",
      "According to eMarketer, photos accounted for 75% of content posted on Facebook worldwide and they are the most engaging type of content on Facebook (87%). Indeed, \"a picture is worth a thousand words\" and now \"photos are worth a million likes.\" Similarly, on Twitter, the tweets with image links get twice as much attention as those without , and video-linked tweets drive up engagement . The ease and naturalness of expression through visual imagery can serve to glean depression-indicators in vulnerable individuals who often seek social support through social media BIBREF7 . Further, as psychologist Carl Rogers highlights, we often pursue and promote our Ideal-Self . In this regard, the choice of profile image can be a proxy for the online persona BIBREF8 , providing a window into an individual's mental health status. For instance, choosing emaciated legs of girls covered with several cuts as profile image portrays negative self-view BIBREF9 .\n",
      "Inferring demographic information like gender and age can be crucial for stratifying our understanding of population-level epidemiology of mental health disorders. Relying on electronic health records data, previous studies explored gender differences in depressive behavior from different angles including prevalence, age at onset, comorbidities, as well as biological and psychosocial factors. For instance, women have been diagnosed with depression twice as often as men BIBREF10 and national psychiatric morbidity survey in Britain has shown higher risk of depression in women BIBREF11 . On the other hand, suicide rates for men are three to five times higher compared to that of the women BIBREF12 .\n",
      "Although depression can affect anyone at any age, signs and triggers of depression vary for different age groups . Depression triggers for children include parental depression, domestic violence, and loss of a pet, friend or family member. For teenagers (ages 12-18), depression may arise from hormonal imbalance, sexuality concerns and rejection by peers. Young adults (ages 19-29) may develop depression due to life transitions, poverty, trauma, and work issues. Adult (ages 30-60) depression triggers include caring simultaneously for children and aging parents, financial burden, work and relationship issues. Senior adults develop depression from common late-life issues, social isolation, major life loses such as the death of a spouse, financial stress and other chronic health problems (e.g., cardiac disease, dementia). Therefore, inferring demographic information while studying depressive behavior from passively sensed social data, can shed better light on the population-level epidemiology of depression.\n",
      "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities – aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement – we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users.\n",
      "We address and derive answers to the following research questions: 1) How well do the content of posted images (colors, aesthetic and facial presentation) reflect depressive behavior? 2) Does the choice of profile picture show any psychological traits of depressed online persona? Are they reliable enough to represent the demographic information such as age and gender? 3) Are there any underlying common themes among depressed individuals generated using multimodal content that can be used to detect depression reliably?\n",
      "\n",
      "Demographic Prediction\n",
      "We leverage both the visual and textual content for predicting age and gender.\n",
      "Prediction with Textual Content:\n",
      "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2 \n",
      "where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset.\n",
      "Prediction with Visual Imagery:\n",
      "Inspired by BIBREF56 's approach for facial landmark localization, we use their pretrained CNN consisting of convolutional layers, including unshared and fully-connected layers, to predict gender and age from both the profile and shared images. We evaluate the performance for gender and age prediction task on INLINEFORM0 and INLINEFORM1 respectively as shown in Table TABREF42 and Table TABREF44 .\n",
      "Demographic Prediction Analysis:\n",
      "We delve deeper into the benefits and drawbacks of each data modality for demographic information prediction. This is crucial as the differences between language cues between age groups above age 35 tend to become smaller (see Figure FIGREF39 -A,B,C) and making the prediction harder for older people BIBREF74 . In this case, the other data modality (e.g., visual content) can play integral role as a complementary source for age inference. For gender prediction (see Table TABREF44 ), on average, the profile image-based predictor provides a more accurate prediction for both the depressed and control class (0.92 and 0.90) compared to content-based predictor (0.82). For age prediction (see Table TABREF42 ), textual content-based predictor (on average 0.60) outperforms both of the visual-based predictors (on average profile:0.51, Media:0.53).\n",
      "However, not every user provides facial identity on his account (see Table TABREF21 ). We studied facial presentation for each age-group to examine any association between age-group, facial presentation and depressive behavior (see Table TABREF43 ). We can see youngsters in both depressed and control class are not likely to present their face on profile image. Less than 3% of vulnerable users between 11-19 years reveal their facial identity. Although content-based gender predictor was not as accurate as image-based one, it is adequate for population-level analysis.\n",
      "\n",
      "Figure 2: The age distribution for depressed and control users in ground-truth dataset\n",
      "\n",
      "Figure 6: Ranking Features obtained from Different Modalities with an Ensemble Algorithm\n",
      "\n",
      "Table 8: Model’s Performance for Depressed User Identification from Twitter using different data modalities\n",
      "----------------------------------------\n",
      "\n",
      "Question: What is the source of the user interaction data? \n",
      "\n",
      "To answer the question, format your response as a `json` object with two fields:\n",
      "\n",
      "\"answer\": A string providing a succinct answer to the question, in your own words.\n",
      "\"evidence\": An array of strings. Each entry should be a full paragraph from the paper. Together, the evidence should serve as a justification for the answer.\n",
      "\n",
      "For instance, for the question \"What baselines did the authors compare against?\", a sample response might be:\n",
      "\n",
      "{\n",
      "  \"answer\": \"BERT and RoBERTa.\"\n",
      "  \"evidence\": [\"We compare our approach against two baselines. In Table 1, we compare against BERT. In Table 2, we compare against RoBERTa. Our findings indicate that our approach improves over both baeslines...\"]\n",
      "}\n",
      "\n",
      "The \"answer\" field should be roughly 39 characters in length.\n",
      "\n",
      "Do not include any text in your response other than the json. If the question is unanswerable given the provided excerpts, respond with the single word \"null\".\n",
      "\n",
      "To repeat, the question is: What is the source of the user interaction data? \n",
      "\n",
      "Answer JSON object:\n"
     ]
    }
   ],
   "source": [
    "subset = 'qasper_abstractive_qa'\n",
    "data = datasets.load_dataset(\"ai2-adapt-dev/science-adapt-4096\",subset)['train']\n",
    "print(data[100]['input'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be shown an abstract from a computer science research paper. Given this abstract, your task is to extract all unique entities with the following types:\n",
      "\n",
      "- \"Task\": Applications, problems to solve, systems to construct. Examples include \"information extraction\", \"machine reading system\", \"image segmentation\".\n",
      "- \"Method\": : Methods, models, systems to use, or tools, components of a system, frameworks. Examples include \"language model\", \"CORENLP\", \"POS parser\".\n",
      "- \"Metric\": Metrics, measures, or entities that can express quality of a system / method. Examples include \"F1\", \"BLEU\", \"Precision\", \"time complexity\".\n",
      "- \"Material\": Data, datasets, resources, Corpus, Knowledge base. Examples include \"image data\", \"speech data\", \"stereo images\", \"CoNLL\", \"Wikipedia\".\n",
      "- \"OtherScientificTerm\": Phrases that are a scientific terms but do not fall into any of the above classes. Examples include \"physical or geometric constraints\", \"qualitative prior knowledge\", \"tree kernel\", \"noise\".\n",
      "- \"Generic\": General terms or pronouns that may refer to a entity but are not themselves informative, often used as connection words. Examples include \"model\", \"approach\", \"them\".\n",
      "\n",
      "Please return the output as a JSON object of the format: {\"type1\" : [\"example_entity\", ...], \"type2\" : [\"example_entity\", ...]}. The keys should be entity types and values should be lists of extracted entities belonging to the corresponding type. Entity types with no matching entities should be assigned an empty array [].\n",
      "\n",
      "For instance, the output might look like: {\"Task\": [\"speech recognition\", ...], \"Method\": [\"Conditional random field\"], \"Material\": [], ...}.\n",
      "\n",
      "Only output the JSON object and do not include any additional text.\n",
      "\n",
      "Abstract:\n",
      "\n",
      "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets.\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# print the 100-th example(input key and output key side-by-side) in a nice format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\": \"Sociability from ego-network on Twitter\", \"evidence\": [\"The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities \\u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile\\u0027s description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \\u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users.\"]}\n"
     ]
    }
   ],
   "source": [
    "print(data[100]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = 'scifact_entailment'\n",
    "data = datasets.load_dataset(\"ai2-adapt-dev/science-adapt-4096\",subset)['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be presented with a citation segment from the section of a research paper. Classify the intent behind this citation by choosing from one of the following categories:\n",
      "\n",
      "- Method: Cites the paper for its methodology or procedure.\n",
      "- Background: Cites the paper to provide background information.\n",
      "- Result: Cites the paper for its findings or results.\n",
      "\n",
      "Your answer should be a single word from the following list of options: [\"Method\", \"Background\", \"Result\"]. Do not include any other text in your response.\n",
      "\n",
      "Citation sentence:\n",
      "\n",
      "A materially stable constitutive model for the simulation of arterial walls has been developed in Holzapfel et al. (2000) (with extensions to the inelastic domain Gasser and Holzapfel, 2002; Holzapfel et al., 2002), where each layer of the artery is modeled as a fiber–reinforced material.\n"
     ]
    }
   ],
   "source": [
    "subset = 'scicite_classification'\n",
    "data = datasets.load_dataset(\"ai2-adapt-dev/science-adapt-4096\",subset)['train']\n",
    "print(data[103]['input'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background\n"
     ]
    }
   ],
   "source": [
    "print(data[103]['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be presented with the title and body text of a computer science research paper. Please write a summary of the work that would be informative for a peer reviewer assessing its quality. Your summary should be 4 sentences long. In your response, include only the summary and no additional text.\n",
      "\n",
      "Paper title:\n",
      "Neural Temporal Logic Programming\n",
      "\n",
      "Paper body:\n",
      "1 INTRODUCTION . Complex time series data is present across many data modalities such as sensors , records , audio , and video data . Typically there are composite events of interest in these time series which are composed of other atomic events in a certain order ( Liu et al. , 1999 ; Chakravarthy et al. , 1994 ; Hinze , 2003 ) . An example is a health symptom that can be observed in a doctor ’ s report . Atomic events , such as patient vitals and medications , and their temporal relations dictate an underlying causal rule leading to the composite event symptom . These rules may be unknown but useful to recover ( Kovačević et al. , 2013 ; Guillame-Bert et al. , 2017 ) . Recent methods leverage the advances in highly parameterized deep architectures to learn latent representations of atomic event data ( Pham et al. , 2017 ; Chen et al. , 2018 ; Choi et al. , 2019 ) , with the increasing availability of large temporal datasets . Methods , such as LSTM ( Hochreiter & Schmidhuber , 1997 ) or Transformer ( Vaswani et al. , 2017 ) based architectures , provide stateof-the-art performance in terms of composite event inference . However , it is uncertain whether the latent representations learn the underlying causal sequence of events or overfit spurious signals in the training data . Having representations faithful to causal mechanisms is advantageous for interpretability , out-of-distribution generalization , and adapting to smaller data sets . Therefore it is important to leverage parametric models that can handle data noise while providing a mechanism to extract explicit temporal rules ( Carletti et al. , 2019 ) . Extracting explicit logic rules has been studied through Inductive Logic Programming ( ILP ) methods ( Muggleton , 1991 ; Muggleton & De Raedt , 1994 ) and have been leveraged in parametric fashions as well ( Yang et al. , 2017 ; Evans & Grefenstette , 2018 ; Rocktäschel & Riedel , 2017 ) . ILP starts with set of background knowledge , consisting of grounded atoms ( i.e . facts which do not contain variables ) such as location ( Braves , Atlanta ) , where the predicate location determines the relationship between the items Braves and Atlanta . There are set of labels from which rules should be learned . The task is to construct a set of rules , when executed over the background knowledge , entail the provided labels . Given the label InLeague ( Braves , NL East ) and the background knowledge ( Figure 1 ILP Input ) as input , a candidate rule is InLeague ( Team , League ) : = Location ( Team , City ) ∧ Division ( City , League ) . Here InLeague ( Team , League ) is the head of the rule consisting of an atom with variables Team , League as items . The body consists of two atoms and when these atoms exist on the background knowledge the rule is evaluated as true.We apply ILP over real world temporal data , however learning such rules poses three key challenges . Temporal Background Knowledge First , ILP methods operate over an existing grounded background knowledge . The temporal case does not have this knowledge when operating over raw time series . For example in a baseball video , grounded atomic events pitch or swing , or grounded predicates such as before ( pitch , swing ) are not explicitly provided . By nature , the video would be labeled with a higher level composite event description , such as `` Player A ’ s home run '' instead of individual atomic events and their corresponding temporal predicates . Such atoms can be extracted using a model in a probabilistic fashion at each time point , and a temporal ILP method should handle this uncertainty . The temporal predicates between these probabilistic atomic events can be applied in a rule-based manner ( ex . t1 < t2 → before ) , but due to the noisy nature of extracted atomic events , the predicate predictions should be robust to consistent noise in the atomic event data . Atomic Event Relevance Second , ILP works learn consistent rules that satisfy a path in the background knowledge given the terms in the labels , such as InLeague ( Braves , NL East ) . The labels are nullary predicates in the temporal case , so the relevant source and target atomic events and predicates to use for rule induction are unknown . In our example , we know from the video we have a label strike , but are not told when it occurred or what other events , such as pitch , swing , and miss are needed to compose a rule for strike . Without a prior on which atomic events to search from , we must consider all pairwise temporal relations between atomic events in the input . This leads to a combinatorial search of all pairwise events for each predicate in the temporal rule body . Multi-Event Labels Third , ILP domains work on disjoint labels , while in time series , multiple composite events could occur in each input . In our baseball video , such as a highlight reel , composite event labels strike , steal and their corresponding atomic events can co-occur in a single video . This further extends the search space of atomic events we consider for each composite event rule . We illustrate these differences in Figure 1 and further discuss these challenges regarding search complexity in Appendix A . To address these challenges , Neural TLP operates on two key steps . Parameter Learning First Neural TLP inputs probabilistic atomic events and learns parameters to infer temporal predicates between atomic events . We represent the atomic event data in an intervalbased representation to efficiently predict all pairwise predicates between atomic events . The inferred predicates are then projected to predict the composite event labels . Structure Learning When the predicate parameters are learned , Neural TLP learns a sparse vector to select the correct rule over the combinatorial space of possible rules . To prune the search space , we use the learned projected weights to select candidate grounded predicates per composite event . We evaluate our method on a synthetic video dataset to empirically test our temporal rule induction performance . Additionally , we apply our framework to provide relevant rules in the healthcare domain , which were verified by doctors . 2 PROBLEM FORMULATION . We define the complete set of atomic events X = { x1 , x2 , . . . , x|X | } along a timeline T . These atomic events can be existing features in time series data or user defined features of interest . A temporal logic rule r ( Xr , Tr ) can be defined as using a subset of N ≤ |X | atomic events Xr = { xu } Nu=1 ⊆ X , and their associated time intervals Tr = { tu } Nu=1 ⊆ T . The time intervals consists of start and end times tu = [ tustart , tuend ] . These intervals indicate durational events and we can also initialize instantaneous events occurring at one time point where tustart = tuend . A rule is evaluated as true if the corresponding atomic events xu are present and are in correct ordering with respect to the intervals tv of other events xv : r ( Xr , Tr ) : = ( ∧ xu∈Xr xu ) ∧ ( ∧ tu , tv∈Tr pi ( tu , tv ) ) The temporal predicates pi ∈ { before , during , after } = P represent a simplified subset of Allen ’ s Temporal Algebra ( Allen , 1983 ) . We simplify the notation of the rules as a conjunction of temporal predicates between observed events , where the event time intervals are implicit : r : = n∧ xu , xv∈Xr pi ( xu , xv ) ( 1 ) For example , the grounded predicate before ( pitch [ 2,2.7 ] , swing [ 3,3.5 ] ) would evaluate to true . These underlying causal rules r induce the composite event labels r → yr seen in the data . Multiple composite events of interest can co-occur during the same time series sample T which we denote as y = { yr } |R| ∈ { 0 , 1 } |R| . Any yr = 1 indicates the latent rule r occurred over T resulting in label yr . While T contains precise atomic event interval information , the observed time series T̃ consists of a sequence of probabilistic atomic events from times [ 1 , T ] . Potentially k different objects T̃ i compose the final time series data T̃ = ⋃k i=1 T̃ i . Examples of objects can be multiple concurrent sensor data , or tracking multiple people moving within a video . Then the input T̃ is formulated as MT ∈ [ 0 , 1 ] k×|X|×T across object , atomic event , and probability dimensions respectively . The temporal ILP task is to recover all underlying rules R given m samples of inputs and labels { ( MTi , yi ) } mi=1 . In Neural TLP this involves learning parameters for the predicates between atomic events and then learning the combination of grounded predicates that induce each r ∈ R . 3 NEURAL TEMPORAL LOGIC PROGRAMMING . Neural TLP operates in two stages . The parameter learning stage learns how to compress the temporal data and learns parameterized temporal predicates . Once these parameters are learned , the structure learning stage learns which conjunctive combination of pairwise atomic event predicates is associated with each composite event label . This conjunction composes the rule r for label yr and is jointly computed for all R. An overview of the framework is presented in Figure 2 . 3.1 PARAMETER LEARNING STAGE . Temporal Compression Starting from the raw probabilistic atomic event data , we first compress the timeline through convolution . This 1D convolution over the temporal dimension compresses and smooths the timeline to mitigate noise from spurious events . Here the convolution kernel K|X |×l of length l is learned per atomic event . We also parameterize α as an extra degree of freedom to scale these convolved scores , which is useful when computing the intermediate predicates downstream . MC ∈ Rk×|X|×t = α · conv_1D ( MT ∈ [ 0 , 1 ] k×|X|×T , K ) ( 2 ) The time information is incorporated by multiplying the time dimension MD into compressed events : MA ∈ Rk×|X|×t = MC ⊙ MD . Here MD has the same dimensions as MC , but the temporal dimension is enumerated from [ 1 , t ] , where MD : , : ,l = l. This can be thought as a positional encoding . For example if we look at the sample compressed scores for a single object i and atomic event j MCi , j,6:10 = [ .01 , .05 , .7 , .7 , .03 ] and MDi , j,6:10 = [ 6 , 7 , 8 , 9 , 10 ] then MAi , j,6:10 = [ .06 , .35 , 5.6 , 6.3 , .3 ] . Intuitively we can see that from MAi , j,6:10 that ( 1 ) atomic event j occurs when the scores are high at 5.6 and 6.3 and that ( 2 ) score 6.3 occurs after score 5.6 due to the multiplied time index . This temporal representation provides a path to compute precise time intervals of atomic event occurrences and define predicates to compare atomic event intervals . Predicate Modeling From the compressed timelines , we determine the temporal predicates between atomic events . These relations are computed in a pairwise manner for all atomic events ∀xu , xv ∈ X occurring in object i through a small network which we call Temporal Predicate Network ( TPN ) . For notation sake here , we represent the atomic event u ’ s timeline for object i as tiu = MAi , u , : ∈ Rt and correspondingly for atomic event v. We denote TPN as gθ ( tiu , t i v ) , which takes pairwise atomic event timelines and predicts a temporal predicate p ∈ P to indicate the relationship between the atomic events . Methods such as Temporal Relation Networks ( Zhou et al. , 2018 ) learn these predicates between video events by sampling frames throughout the video . The timelines can be long in our setting , and events can occur sparsely , making sampling timelines expensive and noisy . To efficiently compute these relations , we would like to recover each event ’ s underlying start and end time intervals . From intervals , we can encode strong inductive biases to predict the predicates.We are working with continuous time series scores in tiu , t i v , so the intervals have to be extracted as the first step in TPN . To compute the start of an event interval , we create a mask to identify the atomic event noise . Those values will be below some small value ϵ , corresponding to noise in the timeline . We learn the convolution scalar α from Equation 2 to scale scores corresponding to active atomic event occurrences above ϵ while keeping scores corresponding to atomic event noise below ϵ . Then the mask is added to the time series , and a min is performed to get the start of the active atomic event interval . Afterwards the min of the mask is subtracted to remove any effect of the mask on the start value . tmask = ( max ( t i u ) + ϵ ) · ( tiu < ϵ ) ( 3 ) ustart = min ( t i u + tmask ) −min ( tmask ) ( 4 ) To get the end of the event interval we simply compute uend = max ( tiu ) since we multiplied the event scores with the time index earlier . This interval computation from the input time series is visualized in Figure 3 . This is computed similarly for the other pairwise event v : [ vstart , vend ] . Given the start and end times for the event pairs u , v , the un-normalized predicate scores are computed as : before ( u , v ) = vstart − uend ( 5 ) after ( u , v ) = ustart − vend ( 6 ) during ( u , v ) = min ( { vend − ustart , uend − vstart } ) ( 7 ) Although we use 3 predicates in our model , similar scores can be developed for more fine grained predicates . Then the values are aggregated as p = [ before ( u , v ) ; during ( u , v ) ; after ( u , v ) ] to compute normalized predictions as p = softmax ( p−βγ ) . Here β and γ and scale and shift parameters learned from data . Our predicates scores assume that intervals for both u and v occur , so if either event doesn ’ t occur we suppress all predicate predictions : supp = min ( { uend − ustart , vend − vstart } ) ( 8 ) pi = min ( { pi , supp } ) ( 9 ) Since we leverage a simple interval representation to compare atomic event objects , we can scale comparing atomic events within the object and between the other k−1 objects : xu ∈ X , xv ∈ ( X×k ) . This second-order interaction information is useful if we want to know if , for example , two events occurred simultaneously within different objects . For a single object i , these relations are computed for all pairwise predicates through TPN in MP ∈ R|X |× ( |X |×k ) ×|P| = Rk×|X|×|X|×|P| . Aggregating over all objects k , we get MQ = [ MP1 ; . . . ; MPk ] ∈ Rk 2×|X|×|X|×|P| . We marginalize over the object dimension to get our final pairwise relation matrix MR = ∑ i MQi , : , : , : ∈ R |X |×|X|×|P| . Composite Event Prediction The final inference step from the pairwise relational predicates to the composite events labels is carried out by fϕ . This is a linear projection function fϕ ( MR ) : = σ ( dropout ( vec ( MR ) ) W ) used to infer the composite event labels ŷ . Here we flatten MR as vec ( MR ) ∈ R|X |·|X |·|P| and regularize it by randomly masking out the grounded predicates ( Srivastava et al. , 2014 ) . This representation is then projected to the label space using W ∈ R ( |X |·|X |·|P| ) ×|y| before passing the un-normalized results through a sigmoid function σ. W learns what grounded relational predicates pi ( xu , xv ) , such as before ( pitch , swing ) , correspond to each composite event label . These weights will also be useful for extracting the rules , in the structure learning stage .\n",
      "\n",
      "4-sentence paper summary:\n"
     ]
    }
   ],
   "source": [
    "subset = 'mup_single_document_summarization'\n",
    "data = datasets.load_dataset(\"ai2-adapt-dev/science-adapt-4096\",subset)['train']\n",
    "print(data[100]['input'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper proposes an end-to-end differentiable strategy (called neural TLP) to learn unknown temporal relations between atomic events (like after(miss, swing), “miss occurs after swing” in the baseball example), subsequently used to predict composite events (like strike). The strategy consists of a cascade of a smoothing stage to filter  out noise from the input time series, an interval time extractor, a stage predicting temporal relations (such as before, after and during) and a linear output layer. Furthermore, the paper proposes a post-hoc procedure to extract propositional logical rules relating atomic events to composite ones from the last layer. The performance of the proposed strategy is evaluated on video recognition (CATER) and healthcare (MIMIC-III) datasets against two baselines, namely a LSTM neural net and a simplified version of the proposed strategy.\n"
     ]
    }
   ],
   "source": [
    "print(data[100]['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science-adapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
